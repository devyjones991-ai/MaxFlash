"""
FIXED LightGBM Training Script - NO Look-Ahead Bias!

This script trains the model CORRECTLY:
1. Uses realistic labels based on CURRENT indicators (no future data)
2. Proper train/test split (chronological, no shuffling)
3. Validates on truly unseen data

Key changes from train_lightgbm.py:
- Uses create_realistic_labels() instead of create_barrier_labels_vectorized()
- No peeking into future bars
- Realistic expected performance
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import ccxt
from datetime import datetime, timedelta
from ml.lightgbm_model import LightGBMSignalGenerator
from ml.labeling_fixed import get_label_distribution, create_realistic_labels
from utils.logger_config import setup_logging

logger = setup_logging()

# Training configuration
TRAINING_CONFIG = {
    'timeframe': '1h',
    'days_back': 180,
    'min_candles': 500,
    'use_new_features': True,
    'num_boost_round': 200,
    'test_size': 0.2,
}

# Top coins
TOP_COINS = [
    "BTC/USDT", "ETH/USDT", "BNB/USDT", "SOL/USDT", "XRP/USDT",
    "ADA/USDT", "AVAX/USDT", "DOGE/USDT", "DOT/USDT", "LINK/USDT",
    "ATOM/USDT", "UNI/USDT", "LTC/USDT", "NEAR/USDT", "ARB/USDT",
    "OP/USDT", "APT/USDT", "SUI/USDT", "INJ/USDT", "FET/USDT",
]

try:
    from utils.universe_selector import get_top_n_pairs
    HAS_UNIVERSE_SELECTOR = True
except ImportError:
    HAS_UNIVERSE_SELECTOR = False


def get_training_coins(n: int = 20) -> list:
    """Get list of coins to train on."""
    if HAS_UNIVERSE_SELECTOR:
        try:
            coins = get_top_n_pairs(n=n, force_refresh=True)
            if coins:
                return coins
        except Exception as e:
            logger.warning(f"Universe selector failed: {e}")
    return TOP_COINS[:n]


def load_historical_data(
    symbol: str,
    timeframe: str = '1h',
    days_back: int = 180,
) -> pd.DataFrame:
    """Load historical data."""
    try:
        exchange = ccxt.binance({
            'enableRateLimit': True,
            'options': {'defaultType': 'future'}
        })

        end_time = datetime.now()
        start_time = end_time - timedelta(days=days_back)
        since = int(start_time.timestamp() * 1000)

        logger.info(f"Fetching {symbol} {timeframe} from {start_time.date()} to {end_time.date()}")

        all_ohlcv = []
        while since < int(end_time.timestamp() * 1000):
            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=1000)
            if not ohlcv:
                break
            all_ohlcv.extend(ohlcv)
            since = ohlcv[-1][0] + 1
            exchange.sleep(exchange.rateLimit / 1000)

        if not all_ohlcv:
            return None

        df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)

        logger.info(f"Loaded {len(df)} candles for {symbol}")
        return df

    except Exception as e:
        logger.error(f"Failed to load {symbol}: {e}")
        return None


def train_lightgbm_fixed(n_coins: int = 20, quick_mode: bool = False):
    """
    Train LightGBM model with FIXED labeling (no look-ahead bias).

    Args:
        n_coins: Number of coins to train on
        quick_mode: If True, use fewer coins and iterations for quick testing
    """
    config = TRAINING_CONFIG

    print("=" * 80)
    print("LIGHTGBM TRAINING - FIXED (NO LOOK-AHEAD BIAS)")
    print("=" * 80)
    print(f"\nConfiguration:")
    print(f"  Timeframe: {config['timeframe']}")
    print(f"  History: {config['days_back']} days")
    print(f"  Labeling: REALISTIC (indicator-based, no future data)")
    print(f"  Features: Enhanced feature engineering")
    print(f"  Boost rounds: {config['num_boost_round']}")

    if quick_mode:
        n_coins = min(n_coins, 5)
        config['num_boost_round'] = 50
        print(f"\n[QUICK MODE] Using {n_coins} coins, {config['num_boost_round']} rounds")

    # Get coins
    coins = get_training_coins(n=n_coins)
    print(f"\n[OK] Training on {len(coins)} coins")

    # Load data for all coins
    all_data = []

    print(f"\n{'='*80}")
    print("LOADING DATA")
    print(f"{'='*80}\n")

    for i, coin in enumerate(coins, 1):
        print(f"[{i:2d}/{len(coins)}] {coin:15s}...", end=" ", flush=True)

        df = load_historical_data(coin, timeframe=config['timeframe'], days_back=config['days_back'])

        if df is None or len(df) < config['min_candles']:
            print("[SKIP] Insufficient data")
            continue

        all_data.append(df)
        print(f"[OK] {len(df)} candles")

    if len(all_data) == 0:
        print("\n[FAILED] No data loaded!")
        return

    # Combine all data
    print(f"\n[OK] Combining data from {len(all_data)} symbols...")
    combined_df = pd.concat(all_data, ignore_index=True)
    combined_df = combined_df.sort_index()

    print(f"[OK] Total training samples: {len(combined_df):,}")

    # Create labels using FIXED method (no look-ahead!)
    print(f"\n{'='*80}")
    print("CREATING REALISTIC LABELS (NO LOOK-AHEAD BIAS)")
    print(f"{'='*80}\n")

    print("[INFO] Using indicator-based labeling:")
    print("  - RSI oversold/overbought zones")
    print("  - MACD crossovers")
    print("  - Bollinger Band positions")
    print("  - Volume spikes")
    print("  - Candle patterns")
    print("\nâœ… NO FUTURE DATA USED!\n")

    # Note: We'll let the LightGBM model create its own labels during training
    # But we need to modify the train() method to use realistic labels

    # Initialize model
    model = LightGBMSignalGenerator()

    print(f"{'='*80}")
    print("TRAINING MODEL")
    print(f"{'='*80}\n")

    # Train with fixed labels
    # We need to temporarily modify the model to use realistic labels
    # This is a workaround - ideally we'd pass label_function as a parameter

    try:
        # Monkey-patch the _create_labels method to use realistic labels
        original_create_labels = model._create_labels

        def _create_labels_realistic(df, **kwargs):
            """Use realistic labels instead of barrier labels."""
            return create_realistic_labels(df)

        model._create_labels = _create_labels_realistic

        metrics = model.train(
            combined_df,
            num_boost_round=config['num_boost_round'],
            test_size=config['test_size'],
            use_new_features=config['use_new_features'],
            use_barrier_labels=False,  # Explicitly disable barrier labels
        )

        # Restore original method
        model._create_labels = original_create_labels

    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()
        return

    print(f"\n{'='*80}")
    print("TRAINING RESULTS")
    print(f"{'='*80}\n")

    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Iterations: {metrics['num_iterations']}")
    print(f"\nClass distribution:")
    for class_name, count in metrics['class_distribution'].items():
        print(f"  {class_name}: {count}")

    print(f"\nTop 10 features:")
    for i, (feature, importance) in enumerate(metrics['top_features'][:10], 1):
        print(f"  {i:2d}. {feature:30s} {importance:>10.0f}")

    # Save model
    models_dir = Path(__file__).parent.parent / "models"
    models_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if quick_mode:
        model_path = models_dir / "lightgbm_quick_fixed.pkl"
    else:
        model_path = models_dir / "lightgbm_latest_fixed.pkl"
        # Also save with timestamp
        archive_path = models_dir / f"lightgbm_fixed_{timestamp}.pkl"
        model.save_model(str(archive_path))
        logger.info(f"Archived model: {archive_path}")

    model.save_model(str(model_path))

    print(f"\n[OK] Model saved: {model_path}")
    print(f"\n{'='*80}")
    print("[OK] TRAINING COMPLETE - NO LOOK-AHEAD BIAS!")
    print(f"{'='*80}\n")

    print("Expected realistic performance:")
    print("  - Win Rate: 45-55% (NOT 100%!)")
    print("  - Profit Factor: 1.2-2.0 (NOT 999!)")
    print("  - This is NORMAL and REALISTIC for trading systems!")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train LightGBM (FIXED - no look-ahead)")
    parser.add_argument('--coins', type=int, default=20, help='Number of coins')
    parser.add_argument('--quick', action='store_true', help='Quick mode (5 coins, 50 rounds)')
    args = parser.parse_args()

    print(f"\nStart time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    train_lightgbm_fixed(n_coins=args.coins, quick_mode=args.quick)

    print(f"\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
